---
title: Temporally coherent person matting trained on fake-motion dataset
permalink: /person-matting
features:
  - "A U-Net-based deep-neural-network method with LSTM blocks and an attention module on skip connections" 
  - "A novel fake-motion algorithm for generating neural-network training video clips from a dataset of images with ground-truth alpha mattes and background videos"
  - "Better than 8 different matting methods according to subjective evaluation"
  - "A motion-estimation-based method for improving the output's temporal stability"
---

### I. Molodetskikh, M. Erofeev, A. Moskalenko, and D. Vatolin

Contact us: 
* <ivan.molodetskikh@graphics.cs.msu.ru>
* <video@compression.ru>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Abstract
We propose a novel neural-network-based method to perform matting of videos depicting people that does not require additional user input such as trimaps. Our architecture achieves temporal stability of the resulting alpha mattes by using motion-estimation-based smoothing of image-segmentation algorithm outputs, combined with convolutional-LSTM modules on U-Net skip connections. We also propose a fake-motion algorithm that generates training clips for the video-matting network
given photos with ground-truth alpha mattes and background videos. We apply random motion to photos and their mattes to simulate movement one would find in real videos and composite the result with the background clips. It lets us train a deep neural network operating on videos in an absence of a large annotated video dataset and provides ground-truth training-clip foreground optical flow for use in loss functions.

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">                   
<div>
 <button class="download-button" role="button" onclick="window.open('https://www.sciencedirect.com/science/article/abs/pii/S10512004
     Read Paper
 </button>
 <p class="download-button-caption">The newest version</p>
<button class="download-button" role="button" onclick="window.open('https://arxiv.org/pdf/2109.04843.pdf')">
    <i class="fa fa-download"></i>
    Download Full Text
</button>
<p class="download-button-caption">The previous version (PDF, 22.2 MB)</p>
</div>


## Key Features
* A U-Net-based deep-neural-network method with LSTM blocks and an attention module on skip connections
* A novel fake-motion algorithm for generating neural-network training video clips from a dataset of images with ground-truth alpha mattes and background videos
* Better than 8 different matting methods according to subjective evaluation powered by [Subjectify.us](https://www.subjectify.us/)
* A motion-estimation-based method for improving the output's temporal stability 

## Architecture of the proposed neural network
The figure below demonstrates the architecture. 
![architecture](/assets/img/papers/person-matting/architecture.JPG)
Some additional details:
* The outputs of the 2nd, 4th, 7th, 14th, and 18th encoder’s blocks of the TorchVision implementation are used for skip connections
* To generate the probability map we used a pretrained DeepLabv3+ image-segmentation network from TorchVision

## Dataset
We developed a novel **fake-motion algorithm** to generate training video clips from image foregrounds and video backgrounds by distorting the foreground image throughout the clip. The fake-motion procedure generates random optical-flow maps at three scales and uses them to warp the input foreground image and alpha mask to produce the foreground clip with the desired frame count.

<div>
    <img src="/assets/img/papers/person-matting/fake_motion.JPG">
    <i><center> Examples of training clips generated by our fake-motion algorithm </center></i>
</div>

To improve network resilience when a person partially leaves the video frame, we added to the optical flow a component that over the clip’s duration shifts the person halfway out of a frame and back, with a probability of $$\frac{1}{3}$$  

## Comparison
We conducted the **subjective evaluation** using [Subjectify.us](https://www.subjectify.us/). In total more than **32,000** pairwise selections were collected and used to fit a Bradley-Terry model.

![subj_comp](/assets/img/papers/person-matting/subjective.png)

Below you can see **objective evaluation** results on clips from [VideoMatting benchmark](http://videomatting.com). The best result is shown in **bold**, the second-best
is underlined and the third-best is shown in _italics_.

<style>
.tablelines table, .tablelines td, .tablelines th {
        border: 0.8px solid black;
        }
</style>
  
|-----------------+-----------------+-----------------+----------------+-----------------+----------------+----------------|
| &nbsp; Method &nbsp; | &nbsp; SSDA &nbsp; | &nbsp; dtSSD &nbsp; | &nbsp; MESSDdt &nbsp; | &nbsp; SSDA &nbsp; | &nbsp; dtSSD &nbsp; | &nbsp; MESSDdt &nbsp; |
|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|
|  | city |  |  | snow |  |  |
| Ours | _69.651_ | **15.314** | **0.695** | 56.338 | 30.240 | _0.662_ |
| Smoothed Prob. Maps | 91.577 | <ins>17.609</ins> | <ins>1.291</ins> | 65.772 | 35.133 | 1.264 |
| FBA Matting | <ins>57.700</ins> | _30.825_ | _1.613_ | <ins>27.113</ins> | <ins>20.881</ins> | <ins>0.423</ins> |
| Deep Image Matting | 97.506 | 47.107 | 3.258 | 59.648 | 41.463 | 2.128 |
| CRGNN | 76.456 | 35.591 | 2.354 | _34.735_ | _27.183_ | 1.244 |
| Sem. Human Matting | 108.393 | 53.086 | 5.696 | 71.844 | 43.689 | 2.595 |
| Late Fusion Matting | **44.766** | 31.621 | 4.152 | **24.602** | **19.484** | **0.341** |
| COSNet | 271.878 | 62.798 | 22.387 | 156.617 | 58.536 | 9.424 |
| MMNet | 154.656 | 62.439 | 13.580 | 347.065 | 143.696 | 58.429 |
|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|
{: .tablelines}
  
The second table shows objective evaluation results on five **VideoMatte240K** test clips.  
  
|-----------------+-----------------+-----------------+----------------|
| &nbsp; Method &nbsp; | &nbsp; SSDA &nbsp; | &nbsp; dtSSD &nbsp; | &nbsp; MESSDdt &nbsp; |
|:---------------:|:---------------:|:---------------:|:---------------:|
| Ours | _84.710_ | _46.792_ | <ins>2.080</ins> |
| Smoothed Prob. Maps |  117.253 | 53.836 | 4.452 |
| FBA Matting | <ins>62.679</ins> | <ins>40.996</ins> | _2.901_ |
| Deep Image Matting | 165.094 | 100.595 | 15.039 |
| CRGNN | 140.095 | 84.434 | 13.900 |
| Sem. Human Matting | 166.325 | 113.648 | 22.300 |
| Late Fusion Matting | **29.146** | **25.459** | **0.845** |
| COSNet | 226.256 | 74.765 | 17.506 |
| MMNet |  445.550 | 156.778 | 75.171 |
|:---------------:|:---------------:|:---------------:|:---------------:|
{: .tablelines}
  
The third table shows objective evaluation results on 100 **fake-motion** clips.  
  
|-----------------+-----------------+-----------------+----------------|
| &nbsp; Method &nbsp; | &nbsp; SSDA &nbsp; | &nbsp; dtSSD &nbsp; | &nbsp; MESSDdt &nbsp; |
|:---------------:|:---------------:|:---------------:|:---------------:|
| Ours | **102.371** | **72.880** | **1.818** |
| Smoothed Prob. Maps |  <ins>113.019</ins> | <ins>79.253</ins> | <ins>3.955</ins> |
| FBA Matting | _114.101_ | _92.686_ | _4.613_ |
| Deep Image Matting | 128.205 | 113.675 | 7.693 |
| CRGNN | 121.416 | 95.899 | 7.015 |
| Sem. Human Matting |  186.980 | 145.235 | 9.685 |
| Late Fusion Matting | 454.597 | 222.736 | 69.992 |
| COSNet | 610.056 | 142.895 | 33.037 |
| MMNet |  222.333 | 124.414 | 15.006 |
|:---------------:|:---------------:|:---------------:|:---------------:|
{: .tablelines}

We believe that the subjective evaluation should play the deciding role in video-matting method evaluation because the existing objective metrics are likely limited in their ability to distinguish temporal coherence.

## Cite us
{% highlight BibTeX %}
@article{MOLODETSKIKH2022103521,
title = {Temporally coherent person matting trained on fake-motion dataset},
journal = {Digital Signal Processing},
volume = {126},
pages = {103521},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103521},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422001385},
author = {Ivan Molodetskikh and Mikhail Erofeev and Andrey Moskalenko and Dmitry Vatolin},
keywords = {Video matting, Semantic person matting, Semantic segmentation, Data augmentation, Temporal smoothing, Deep learning},
abstract = {We propose a novel neural-network-based method to perform matting of videos depicting people that does not require additional user input such as trimaps. Our architecture achieves temporal stability of the resulting alpha mattes by using motion-estimation-based smoothing of image-segmentation algorithm outputs, combined with convolutional-LSTM modules on U-Net skip connections. We also propose a fake-motion algorithm that generates training clips for the video-matting network given photos with ground-truth alpha mattes and background videos. We apply random motion to photos and their mattes to simulate movement one would find in real videos and composite the result with the background clips. It lets us train a deep neural network operating on videos in an absence of a large annotated video dataset and provides ground-truth training-clip foreground optical flow for use in loss functions.}
}
{% endhighlight %}

## Contact us

For questions and propositions, please contact us: <ivan.molodetskikh@graphics.cs.msu.ru>, <video@compression.ru>

## See also 
* [VideoMatting benchmark](http://videomatting.com)
* [Subjectify.us](https://www.subjectify.us/)
* [MSU benchmarks](https://videoprocessing.ai/benchmarks/)

## References

1. M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen, Mobilenetv2: inverted
residuals and linear bottlenecks, in: The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.
2. X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-k. Wong, W.-c. Woo, Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, Advances in Neural Information Processing Systems, vol. 28, Curran Associates, Inc., 2015, pp. 802–810.
3. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, in: Advances in Neural Information Processing Systems, 2017, pp. 5998–6008.
4. L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam, Encoder-decoder with atrous separable convolution for semantic image segmentation, in: Proceedings of the European Conference on Computer Vision (ECCV), 2018.
5.  [AISegment dataset](https://github.com/aisegmentcn/matting_human_datasets/tree/1829b5f722024d29b780993f06b45ea3f47ba777), 2019.
6.  X. Shen, X. Tao, H. Gao, C. Zhou, J. Jia, Deep automatic portrait matting, in: European Conference on Computer Vision, Springer, 2016, pp. 92–107.
7.  N. Xu, B. Price, S. Cohen, T. Huang, Deep image matting, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
8.  M. Forte, F. Pitié, F , B, alpha matting, preprint, arXiv:2003.07711, 2020.
9.  X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, F. Porikli, See more, know more: unsupervised video object segmentation with co-attention siamese networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
10. S. Seo, S. Choi, M. Kersner, B. Shin, H. Yoon, H. Byun, S. Ha, Towards realtime automatic portrait matting on mobile devices, preprint, arXiv:1904.03816, 2019.
11. Q. Chen, T. Ge, Y. Xu, Z. Zhang, X. Yang, K. Gai, Semantic human matting, in: Proceedings of the 26th ACM International Conference on Multimedia, 2018, pp. 618-626.
12. Y. Zhang, L. Gong, L. Fan, P. Ren, Q. Huang, H. Bao, W. Xu, A late fusion CNN for digital matting, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
13. T. Wang, S. Liu, Y. Tian, K. Li, M.-H. Yang, Video matting via consistencyregularized graph neural networks, in: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 4902–4911.
14. R.A. Bradley, M.E. Terry, Rank analysis of incomplete block designs: I. The
method of paired comparisons, Biometrika 39 (3/4) (1952) 324–345.

